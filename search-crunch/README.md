# Cloudera Search - Crunch Indexer Tool

## Building

This step builds the software from source. It also runs the unit tests.

```bash
git clone http://github.mtv.cloudera.com/CDH/search
cd search
#git tag # list available releases
#git checkout master
git checkout cdh5-1.0.0 # or whatever the latest version is
mvn clean package -pl search-crunch # This can take several minutes
cd search-crunch
ls target/search-crunch-*-job.jar
```

## Getting Started

The steps below assume you have already built the software as described above.
In addition, below we assume a working MapReduce or Spark cluster, for example as installed by Cloudera Manager.

## CrunchIndexerTool

`CrunchIndexerTool` is a Spark or MapReduce ETL batch job that pipes data from (splitable or non-splitable) HDFS files into Apache  Solr,  and  along the way runs the data through a Morphline  for extraction  and transformation. The program is
designed for flexible, scalable and fault-tolerant batch ETL pipeline jobs. It is implemented as an  Apache  Crunch  pipeline and as such can run
on either the Apache Hadoop MapReduce or Apache Spark execution engine. More details are available through the command line help:

<pre>
$ hadoop jar target/search-crunch-*-job.jar org.apache.solr.crunch.CrunchIndexerTool --help

MapReduceUsage: hadoop jar search-crunch-*-job.jar org.apache.solr.crunch.CrunchIndexerTool [MapReduceGenericOptions]...
        [--input-file-list URI] [--input-file-format FQCN]
        [--input-file-projection-schema FILE]
        [--input-file-reader-schema FILE] --morphline-file FILE
        [--morphline-id STRING] [--pipeline-type STRING] [--xhelp]
        [--mappers INTEGER] [--log4j FILE] [--chatty]
        [HDFS_URI [HDFS_URI ...]]

SparkUsage: spark-submit [SparkGenericOptions]... --master local|yarn --deploy-mode client|cluster 
--class org.apache.solr.crunch.CrunchIndexerTool search-crunch-*-job.jar
        [--input-file-list URI] [--input-file-format FQCN]
        [--input-file-projection-schema FILE]
        [--input-file-reader-schema FILE] --morphline-file FILE
        [--morphline-id STRING] [--pipeline-type STRING] [--xhelp]
        [--mappers INTEGER] [--log4j FILE] [--chatty]
        [HDFS_URI [HDFS_URI ...]]

Spark or MapReduce ETL batch job  that  pipes data from (splitable or non-
splitable) HDFS files into Apache Solr,  and  along  the way runs the data
through a Morphline  for  extraction  and  transformation.  The program is
designed for flexible,  scalable  and  fault-tolerant  batch  ETL pipeline
jobs. It is implemented as an Apache  Crunch  pipeline and as such can run
on either the Apache Hadoop MapReduce or Apache Spark execution engine.

The program proceeds in several consecutive phases, as follows: 

1) Randomization phase: This (parallel) phase  randomizes the list of HDFS
input files in  order  to  spread  ingestion  load  more  evenly among the
mapper tasks of the subsequent phase. This phase is only executed for non-
splitables files, and skipped otherwise.

2) Extraction phase: This (parallel)  phase  emits  a series of HDFS input
file paths (for non-splitable files)  or  a  series  of input data records
(for splitable files), in the form of a Crunch PCollection. 

3)  Morphline  phase:  This  (parallel)  phase  takes  the  items  of  the
PCollection generated by  the  previous  phase,  and  uses  a Morphline to
extract  the  relevant  content,  transform  it  and  load  zero  or  more
documents into Solr. The  ETL  functionality  is flexible and customizable
using chains of arbitrary morphline  commands  that  pipe records from one
transformation command to another. Commands  to  parse and transform a set
of standard data formats  such  as  Avro,  Parquet,  CSV, Text, HTML, XML,
PDF, MS-Office, etc. are provided  out  of  the box, and additional custom
commands and parsers for additional file  or  data formats can be added as
custom morphline commands. Any kind  of  data  format can be processed and
any kind output  format  can  be  generated  by  any  custom Morphline ETL
logic. Also, this phase  can  be  used  to  send  data  directly to a live
SolrCloud cluster (via the loadSolr morphline command).

The program is  implemented  as  a  Crunch  pipeline  and  as  such Crunch
optimizes the logical phases  mentioned  above  into an efficient physical
execution  plan  that  runs   a   single   mapper-only   job,  or  as  the
corresponding Spark equivalent.

Fault Tolerance: Task attempts  are  retried  on  failure per the standard
MapReduce or Spark semantics. If the whole  job fails you can retry simply
by rerunning the program again using the same arguments.

CrunchIndexerOptions:
  HDFS_URI               HDFS URI of  file  or  directory  tree to ingest.
                         (default: [])
  --input-file-list URI  Local URI or HDFS  URI  of  a  UTF-8 encoded file
                         containing a list  of  HDFS  URIs  to ingest, one
                         URI per line in  the  file.  If '-' is specified,
                         URIs are read from  the  standard input. Multiple
                         --input-file-list arguments can be specified.
  --input-file-format FQCN
                         The Hadoop FileInputFormat to  use for extracting
                         data from splitable HDFS  files.  Can  be a fully
                         qualified Java  class  name  or  one  of ['text',
                         'avro',  'avroParquet'].   If   this   option  is
                         present the extraction phase  will  emit a series
                         of input data  records  rather  than  a series of
                         HDFS file paths.
  --input-file-projection-schema FILE
                         Relative or absolute path to  an Avro schema file
                         on the local file  system.  This  will be used as
                         the projection schema for Parquet input files.
  --input-file-reader-schema FILE
                         Relative or absolute path to  an Avro schema file
                         on the local file  system.  This  will be used as
                         the reader  schema  for  Avro  or  Parquet  input
                         files.     Example:      src/test/resources/test-
                         documents/strings.avsc
  --morphline-file FILE  Relative or absolute path to  a local config file
                         that contains one  or  more  morphlines. The file
                         must be UTF-8  encoded.  It  will  be uploaded to
                         each remote  task.  Example:  /path/to/morphline.
                         conf
  --morphline-id STRING  The identifier of  the  morphline  that  shall be
                         executed  within   the   morphline   config  file
                         specified  by   --morphline-file.   If   the   --
                         morphline-id option is  ommitted  the first (i.e.
                         top-most) morphline  within  the  config  file is
                         used. Example: morphline1
  --pipeline-type STRING
                         The engine to use for  executing  the job. Can be
                         'mapreduce' or 'spark'. (default: mapreduce)
  --xhelp, --help, -help
                         Show this help message and exit
  --mappers INTEGER      Tuning knob that indicates  the maximum number of
                         MR mapper tasks to use.  -1 indicates use all map
                         slots available on  the  cluster.  This parameter
                         only  applies   to   non-splitable   input  files
                         (default: -1)
  --log4j FILE           Relative or absolute  path  to a log4j.properties
                         config file on the  local  file system. This file
                         will be uploaded  to  each  remote task. Example:
                         /path/to/log4j.properties
  --chatty, --verbose    Turn on verbose output. (default: false)


SparkGenericOptions:     To print all options run 'spark-submit --help'

MapReduceGenericOptions: Generic options supported are
  --conf &lt;configuration file&gt;
                         specify an application configuration file
  -D &lt;property=value&gt;    use value for given property
  --fs &lt;local|namenode:port&gt;
                         specify a namenode
  --jt &lt;local|jobtracker:port&gt;
                         specify a job tracker
  --files &lt;comma separated list of files&gt;
                         specify comma separated files to be copied to the map reduce cluster
  --libjars &lt;comma separated list of jars&gt;
                         specify comma separated jar files to include in the classpath.
  --archives &lt;comma separated list of archives&gt;
                         specify comma separated  archives  to  be  unarchived  on  the compute
                         machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

Examples: 

# Prepare - Copy input files into HDFS:
hadoop fs -copyFromLocal src/test/resources/test-documents/hello1.txt hdfs:/user/systest/input/

# MapReduce on Yarn - Ingest text file line by line into Solr:
hadoop \
  --config /etc/hadoop/conf.cloudera.YARN-1 \
  jar target/search-crunch-*-job.jar org.apache.solr.crunch.CrunchIndexerTool \
  -D 'mapred.child.java.opts=-Xmx500m' \
  -D morphlineVariable.ZK_HOST=$(hostname):2181/solr \
  --files src/test/resources/test-documents/string.avsc \
  --morphline-file src/test/resources/test-morphlines/loadSolrLineWithOpenFile.conf \
  --pipeline-type mapreduce \
  --chatty \
  --log4j src/test/resources/log4j.properties \
  /user/systest/input/hello1.txt

# Spark in Local Mode (for rapid prototyping) - Ingest into Solr:
spark-submit \
  --master local \
  --deploy-mode client \
  --class org.apache.solr.crunch.CrunchIndexerTool \
  target/search-crunch-*-job.jar \
  -D morphlineVariable.ZK_HOST=$(hostname):2181/solr \
  --morphline-file src/test/resources/test-morphlines/loadSolrLineWithOpenFile.conf \
  --pipeline-type spark \
  --chatty \
  --log4j src/test/resources/log4j.properties \
  /user/systest/input/hello1.txt

# Spark on Yarn in Client Mode (for testing) - Ingest into Solr:
Same as above, except replace '--master local' with '--master yarn'

# View the yarn executor log files (there is no GUI yet):
yarn logs --applicationId $application_XYZ

# Spark on Yarn in Cluster Mode (for production) - Ingest into Solr:
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --class org.apache.solr.crunch.CrunchIndexerTool \
  --files src/test/resources/log4j.properties,src/test/resources/test-morphlines/loadSolrLineWithOpenFile.conf \
  target/search-crunch-*-job.jar \
  -D morphlineVariable.ZK_HOST=$(hostname):2181/solr \
  --morphline-file loadSolrLineWithOpenFile.conf \
  --pipeline-type spark \
  --chatty \
  --log4j log4j.properties \
  /user/systest/input/hello1.txt
</pre>
  